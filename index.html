<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
    <meta name="author" content="Yaqiang Cao" />
    <meta name="keyworkds" content="Yaqiang Cao, Homepage" />
    <meta name="description" content="Homepage of Yaqiang Cao" />
    <meta name="keywords" content="Yaqiang Cao" />
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" href="style.css" type="text/css" />
    <title>MCCS2023 Dataset</title>
</head>

<body data-feedly-mini="yes">
    <div id="layout-content">

        <script type="text/javascript">
            < !--
                function toggleBibtex(articleid) {
                    var bib = document.getElementById(articleid);
                    if (bib.style.display == "none") {
                        bib.style.display = "";
                    }
                    else {
                        bib.style.display = "none";
                    }
                }
                - >

        </script>

        <script type="text/javascript">
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-40926388-1']);
            _gaq.push(['_trackPageview']);
            (function () {
                var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
                ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
            })();

        </script>

        <table class="imgtable">
            <tr>
                <div id="toptitle">
                    <h1 style="text-align:center">
                        Mandarin Chinese Cued Speech Dataset (MCCS2023)
                    </h1>
                </div>
                <p style="text-align:center">
                    The Chinese University of Hongkong, Shenzhen
                    <br />
                    Shenzhen Research Institute of Big Data
                    <br />
                    The Hong Kong University of Science and Technology (Guangzhou)
                </p>
            </tr>
        </table>
<div style=" text-align:center"><img src="example2.png" /></div>

        <h2>Introduction</h2>
        <div>
            <p>
            This MCCS2023 dataset is the first large scale Mandarin Chinese Cued Speech dataset. This dataset covers 23 major categories of scenarios and 72 subcategories of scenarios, covering most of the communication scenarios such as work, study and so on. It is recorded by 4 skilled Cued Speech performers with portable cameras on the mobile phones. The Cued Speech videos are recorded with 30fps and 1280x720 format.
            </p>
        </div>
        <p>There are two kind of data annotation available:</p>
        <ol>
            <li >
Continuious video annotation with ELAN .</li><li >
Discrete audio annotations with Praat.</li>
        </ol>
        <h2>
            DownLoad
        </h2>

                <p>This MCCS2023 dataset contains 1000 Chinese Cued Speech sentences. We currently only provide it to universities and research institutions for research purposes. Please complete the following steps to obtain the dataset:
                </p>
        <ol>
            <li >
                Download <a target="_black" href="Release Agreement-MCCS.pdf" style="color:red">The MCCS2023 Dataset Release Agreement</a>
            </li>
                        <li>
                            Complete the agreement appropriately after reading it carefully. Currently the agreement is only accepted to be signed by a <a style="color:red">full-time staff member</a> (students are not accepted)
            </li>
                        <li>
                            Please provide the signed agreement and send it to (wentaolei@link.cuhk.edu.com) and CC to Prof. Liu (avrillliu@hkust-gz.edu.cn)
            </li>

        </ol>

        <td align="bottom" colspan="1" style="margin-top:5px;">
            <h2>References</h2>
            <div style="height: 470px; overflow: auto;">
                <p>If you use this MCCS dataset in your research, please consider citing the following papers:</p>
                <ol>                                          <li>   Objective Hand Complexity Comparison between Two Mandarin Chinese Cued Speech Systems.
Li Liu, Gang Feng, Xiaoxi Ren, Xianping Ma  <i><b>International Symposium on Chinese Spoken Language Processing (ISCSLP)</b></i> (2022)
                        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8903053">LINK</a>
                    </li>
                                <li>   An Attention Self-supervised Contrastive Learning based Three-stage Model for Hand Shape Feature Representation in Cued Speech.
Jianrong Wang, Nan Gu, Mei Yu, Xuewei Li, Qiang Fang, Li Liu*  <i><b>Conference of the International Speech Communication Association (Interspeech)</b></i> (2021)
                        <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/wang21f_interspeech.pdf">LINK</a>
                    </li>
                                                    <li>    Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition.
Jianrong Wang, Ziyue Tang, Xuewei Li, Mei Yu, Qiang Fang, Li Liu*  <i><b>Conference of the International Speech Communication Association (Interspeech)</b></i> (2021)
                        <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/wang21v_interspeech.pdf">LINK</a>
                    </li>
                    <li>
                        Re-synchronization using the Hand Preceding Model for Multi-modal Fusion in Automatic Continuous Cued Speech Recognition.
Li Liu, Gang Feng, Denis Beautemps, Xiao-Ping Zhang.
<i><b>IEEE Transactions on Multimedia</b></i> (2020)
                        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9016100&tag=1">LINK</a>
                    </li>
                                      <li>
                    A Pilot Study on Mandarin Chinese Cued Speech. Li Liu, Gang Feng. <i><b>American Annals of the Deaf</b></i> (2019)
                        <a href="https://muse.jhu.edu/article/745662/pdf">LINK</a>
                    </li>

                    </li>
                                        <li>    A Novel Resynchronization Procedure for Hand-lips Fusion applied to Continuous French Cued Speech Recognition.
Li Liu, Gang Feng, Denis Beautemps, Xiao-Ping Zhang  <i><b>European Signal Processing Conference (EUSIPCO)</b></i> (2019)
                        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8903053">LINK</a>
                    </li>
                                          <li>   Visual Recognition of Continuous Cued Speech Using a Tandem CNN-HMM Approach.
Li Liu, Thomas Hueber, Gang Feng, Denis Beautemps  <i><b>Conference of the International Speech Communication Association (Interspeech)</b></i> (2019)
                        <a href="https://www.isca-speech.org/archive_v0/Interspeech_2018/pdfs/2434.pdf">LINK</a>
                    </li>
                                          <li>   Automatic Detection of the Temporal Segmentation of Hand Movements in British English Cued Speech.
Li Liu, Jianze Li, Gang Feng, Xiao-Ping Zhang  <i><b>Conference of the International Speech Communication Association (Interspeech)</b></i> (2019)
                        <a href="https://www.isca-speech.org/archive_v0/Interspeech_2018/pdfs/2434.pdf">LINK</a>
                    </li>
                                              <li>   Automatic Temporal Segmentation of Hand Movements for Hand Positions Recognition in French Cued Speech.
Li Liu, Gang Feng, Denis Beautemps  <i><b>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</b></i> (2018)
                        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462090">LINK</a>
                    </li>
                                <li>                         Inner Lips Feature Extraction based on CLNF with Hybrid Dynamic Template for Cued Speech.
Li Liu, Gang Feng, Denis Beautemps. <i><b>EURASIP Journal on Image and Video Processing</b></i> (2017)
                        <a href="https://muse.jhu.edu/article/745662/pdf">LINK</a>
                    </li>

                </ol>
            </div>
        </td>



        <h2>Contact</h2>
                    <p>If you have any questions about the dataset and our papers, please feel free to contact us:</p>
        <ul>
            <li>
                <p><a href="mailto:avrillliu@hkust-gz.edu.cn">Prof. Li Liu</a>  avrillliu@hkust-gz.edu.cn</p>
            </li>
            <li>
                <p><a href="mailto:wentaolei@link.cuhk.edu.cn">Wentao Lei</a> wentaolei@link.cuhk.edu.cn</p>
            </li>
        </ul>

        <p>You can also visit <a href="https://liliu-avril.github.io/">Homepage of Prof.Liu </a>for more details about our group and research topics.</p>



        <!-- BEGIN: Powered by Supercounters.com -->
        <center>
            <script type="text/javascript" src="//widget.supercounters.com/ssl/flag.js"></script>
            <script type="text/javascript">sc_flag(1642297, "FFFFFF", "000000", "cccccc", 4, 12, 0, 0)</script>
        </center>
        <!-- END: Powered by Supercounters.com -->

        <div id="footer">
            <div id="footer-text">
                </br>Last updated at 2023-02-18.
            </div>
        </div>
    </div>


</body>

</html>
